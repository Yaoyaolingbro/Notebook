# 机器学习基础
<!-- prettier-ignore-start -->
!!! Note "导语"
    这里我会记录一些机器学习的基本概念和入门知识。
<!-- prettier-ignore-end -->



## 基本概念《机器学习》

1. 模型指全局性结果，模式指局部结果。
2. 记录的集合称为一个"数据集" (data set) ，其中每条记录是关于一个事件或对象(这里是一个西瓜)的描述，称为一个"示例" (instance) 或"样本" (sample). 反映事件或对象在某方面的表现或性质的事项，例如"色泽""根蒂" "敲声"，称为"属性" (attribute) 或"特征" (feature); 属性上的取值，例如"青绿" "乌黑"，称为"属性值" (attribute value). 属性张成的空间称为"属性空间" (attribute space) "样本空间" (sample space) 或"输入空间"例如我们把"色泽" "根蒂" "敲声"作为三个坐标轴，则它们张成一个用于描述西瓜的三维空间，每个西瓜都可在这个空间中找到自己的坐标位置.由于空间中的每个点对应一个坐标向量，因此我们也把一个示例称为一个"特征向量" (feature vector). `以西瓜为例`
3. 从数据中学得模型的过程称为”学习“或者”训练“。要建立”预测“（prediction）模型，我们首先需要”标记“（label），拥有标记信息的示例叫做”样例“。我们若预测的是离散值，则是”分类“（classification）；若是连续值，则是“回归”。其中关于“二分类任务：通常称其中一类为”正类“（positive class），另外一类为”反类“或”负类“（negative class）。对于分类和回归而言是**监督学习**.（supervised learning）
4. 当然我们也可以对对象做”聚类“（clustering），即对对象进行分组，每组称为”簇“（cluster）如：浅色瓜、深色瓜、大瓜小瓜等等。 通常来说做聚类的学习过程中无标记信息，我们称为**无监督学习**（unsupervised learning）
5. 机器学习的目的是为了更好适用于”新样本“，而不仅仅是在已有样本上得到好的结果。这种能力我们称为”泛化“（generalization）能力，具有强泛化能力的模型通常在整个样本空间上都工作得很好。我们一般假设样本空间全体样本服从一个未知”分布“（distribution）。我们获得的每个样本都是独立地从这个分布上采样获得的，即独立同分布（independent and identically distributed，简称$i,i,d$.)一般而言训练样本越多，我们通过学习得到的模型泛化能力越强。
6. 归纳偏好：相当于找到一条穿过所有训练样本点的区线。我们更偏好光滑的区线。但我们根据已有的数据得到的曲线并不一定对所有空间内所有样本适用，比如：![](graph\Snipaste_2023-07-11_14-59-25.png)

> 简称：NFL（no free lunch theorem）定律。所以也说明我们要针对学习的问题来谈论算法的优劣

## Markov
<!-- prettier-ignore-start -->
!!! Note "导语"
    马尔可夫链可谓是在太多地方出现了，由于在b站上刷到了一个简明有趣的视频（不出意外是YouTube上搬运的，[视频原地址](https://www.youtube.com/watch?v=i3AkTO9HLXo)）故而记录下来。
<!-- prettier-ignore-end -->




