# Activation Function
<!-- prettier-ignore-start -->
!!! note "List"
    - [ ] [Softmax](https://en.wikipedia.org/wiki/Softmax_function)
    - [ ] [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))
    - [ ] [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)
    - [ ] [GELU](https://en.wikipedia.org/wiki/Gaussian_error_linear_unit)
    - [ ] [Tanh](https://en.wikipedia.org/wiki/Hyperbolic_function)
    - [ ] [Swish](https://en.wikipedia.org/wiki/Swish_function)
<!-- prettier-ignore-end -->

<!-- prettier-ignore-start -->
???+ quote "Resource"
    [一文汇总](https://www.jiqizhixin.com/articles/2021-02-24-7)
    
<!-- prettier-ignore-end -->